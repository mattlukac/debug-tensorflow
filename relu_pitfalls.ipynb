{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When keeping it ReLU goes wrong\n",
    "\n",
    "RuLU activations are often recommended for regression-based neural network tasks.\n",
    "In certain scenarios, even very simple ones as we will see below, ReLU activations\n",
    "can actually impede the learning process of a neural net. \n",
    "\n",
    "Here, we construct a simple feed-forward network and ask it to learn the identity map on the unit interval.\n",
    "That is, the network is expected to approximate the function $f(x) = x$ for $x \\in (0,1)$. \n",
    "To do this we design a network with 1 input node, 1 output node, and a single hidden layer with 2 nodes.\n",
    "This results in 4 weight and 3 bias parameters, where first weight matrix is a column vector and the second is a row vector, each of length 2. \n",
    "\n",
    "That is, for input $x$, let $W_1 \\in \\mathbb{R}^{2\\times 1}$ and $W_2 \\in \\mathbb{R}^{1\\times 2}$ be the first and second weight matrices with corresponding bias vectors $b_1 \\in \\mathbb{R}^2$ and $b_2 \\in \\mathbb{R}$. Then the network is the map $(0,1) \\to \\mathbb{R}^2 \\to \\mathbb{R}$ defined by\n",
    "\n",
    "$$\n",
    "    x \\mapsto \\text{relu}(W_1 x + b_1) \\mapsto \\text{relu}(W_2 \\text{relu}(W_1 x + b_1) + b_2)\n",
    "$$\n",
    "\n",
    "We will see that the network defined above will be insufficient for the assigned task, and that in this case it is better to replace ReLU with linear activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "n_samples = 2000\n",
    "n_features = 1\n",
    "x = np.ones((n_samples, n_features))\n",
    "np.random.seed(23)\n",
    "y = np.random.uniform(size=n_samples)\n",
    "for idx in range(n_samples):\n",
    "    x[idx] *= y[idx]\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=23)\n",
    "\n",
    "# construct the network\n",
    "def make_network(activations='relu'):\n",
    "    model = tf.keras.Sequential()\n",
    "    # connect input node to hidden layer\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        2, \n",
    "        activations, \n",
    "        input_shape=(n_features, ), \n",
    "        bias_initializer=tf.keras.initializers.Zeros())\n",
    "             )\n",
    "    # connect hidden layer to output node\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        1, \n",
    "        activations, \n",
    "        bias_initializer=tf.keras.initializers.Zeros())\n",
    "             )\n",
    "    # extract the initial weights\n",
    "    initial_weights = [w.numpy() for w in model.weights]\n",
    "    \n",
    "    return model, initial_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial ReLU model weights:\n",
      "  W_1 = [-0.16988075 -1.1390355 ]\n",
      "  b_1 = [0. 0.]\n",
      "  W_2 = [ 1.2553526  -0.45677203]\n",
      "  b_2 = [0.]\n",
      "Initial linear model weights:\n",
      "  W_1 = [-0.16988075 -1.1390355 ]\n",
      "  b_1 = [0. 0.]\n",
      "  W_2 = [ 1.2553526  -0.45677203]\n",
      "  b_2 = [0.]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(23)\n",
    "relu_model, relu_init = make_network('relu')\n",
    "\n",
    "tf.random.set_seed(23)\n",
    "linear_model, linear_init = make_network('linear')\n",
    "\n",
    "# print weights\n",
    "print('Initial ReLU model weights:')\n",
    "print('  W_1 =', relu_init[0][0])\n",
    "print('  b_1 =', relu_init[1])\n",
    "print('  W_2 =', relu_init[2][:,0])\n",
    "print('  b_2 =', relu_init[3])\n",
    "print('Initial linear model weights:')\n",
    "print('  W_1 =', linear_init[0][0])\n",
    "print('  b_1 =', linear_init[1])\n",
    "print('  W_2 =', linear_init[2][:,0])\n",
    "print('  b_2 =', linear_init[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see both models have the same initial state, the only difference is their activations.\n",
    "Let's see how they learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 0s 725us/step - loss: 0.4887\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 0s 671us/step - loss: 0.4887\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 0s 691us/step - loss: 0.4887\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 0s 688us/step - loss: 0.4887\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 0s 698us/step - loss: 0.4887\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 0s 661us/step - loss: 0.4887\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 0s 690us/step - loss: 0.4887\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 0s 711us/step - loss: 0.4887\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 0s 675us/step - loss: 0.4887\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 0s 687us/step - loss: 0.4887\n"
     ]
    }
   ],
   "source": [
    "# train network with ReLU activations\n",
    "relu_model.compile(optimizer='adam', loss='mae')\n",
    "relu_history = relu_model.fit(x_train, y_train, epochs=10, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 0s 730us/step - loss: 0.2267\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 0s 677us/step - loss: 0.1088\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 0s 681us/step - loss: 0.0799\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 0s 672us/step - loss: 0.0544\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 0s 669us/step - loss: 0.0258\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 0s 682us/step - loss: 0.0022\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 0s 673us/step - loss: 0.0010\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 0s 665us/step - loss: 8.1417e-04\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 0s 658us/step - loss: 4.9381e-04\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 0s 834us/step - loss: 5.3769e-04\n"
     ]
    }
   ],
   "source": [
    "# train network with linear activations\n",
    "linear_model.compile(optimizer='adam', loss='mae')\n",
    "linear_history = linear_model.fit(x_train, y_train, epochs=10, batch_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's going on here? Well, we can actually compute the activations of each network using the initial weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    negatives = (x < 0)\n",
    "    x[negatives] = 0.0\n",
    "    return x\n",
    "\n",
    "def compute_activations(x, weights, activations):\n",
    "    assert activations in ['relu', 'linear']\n",
    "    # get weights\n",
    "    W1 = weights[0]\n",
    "    b1 = weights[1]\n",
    "    W2 = weights[2]\n",
    "    b2 = weights[3]\n",
    "    \n",
    "    hidden = W1.dot(x) + b1\n",
    "    \n",
    "    if activations == 'relu':\n",
    "        hidden = relu(hidden)\n",
    "        final = relu(hidden.dot(W2) + b2)\n",
    "    else:\n",
    "        final = hidden.dot(W2) + b2\n",
    "        \n",
    "    return hidden, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU model:\n",
      "  hidden layer activations: [0. 0.]\n",
      "  final layer activations: [0.]\n",
      "Linear model:\n",
      "  hidden layer activations: [-0.08494037 -0.56951773]\n",
      "  final layer activations: [0.15350965]\n"
     ]
    }
   ],
   "source": [
    "relu_hidden, relu_final = compute_activations(0.5, relu_init, 'relu')\n",
    "linear_hidden, linear_final = compute_activations(0.5, linear_init, 'linear')\n",
    "\n",
    "print('ReLU model:')\n",
    "print('  hidden layer activations:', relu_hidden[0])\n",
    "print('  final layer activations:', relu_final[0])\n",
    "print('Linear model:')\n",
    "print('  hidden layer activations:', linear_hidden[0])\n",
    "print('  final layer activations:', linear_final[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the initial weights for the input, $W_1$, are all negative and every input $x$ is positive,\n",
    "none of the hidden nodes will be activated with ReLU activations. Since there are no activations\n",
    "in the hidden layer, the value to be activated in the final layer is zero, hence no final layer activations.\n",
    "\n",
    "On the other hand, with linear activations the final activation value is simply $x W_2 W_1$ which is an inner product.\n",
    "Hence we can write the final activation as \n",
    "\n",
    "$$\n",
    "    x |W_1| |W_2| \\cos\\theta\n",
    "$$\n",
    "\n",
    "where $\\theta$ is the angle between the vectors $W_1$ and $W_2$.\n",
    "This observation also sheds light on the ReLU case; even if all hidden units are activated the final layer\n",
    "will only be activated whenever $\\theta$ is acute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
